{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYj-1ZrL-Bpl"
      },
      "source": [
        "INSTALLING PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MlHh_kB3nz6I"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv langchain langchain_community openai tiktoken langchain_huggingface pymupdf faiss-cpu -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ZkWtpf-AmQ"
      },
      "source": [
        "INITIALIZE OPENAI API KEYS AND BASE URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "axDYUIT32q9D"
      },
      "outputs": [],
      "source": [
        "from dotenv import dotenv_values\n",
        "import openai\n",
        "\n",
        "env_vars = dotenv_values('.env.txt')\n",
        "\n",
        "openai_api_key = env_vars['OPENAI_API_KEY']\n",
        "openai_base_url = env_vars['OPENAI_BASE_URL']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYLK9Vev-VMF"
      },
      "source": [
        "CREATE CHAT INSTANCE WITH OPENAI MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fk2KjOVSraNW"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "model_version = 'gpt-3.5-unfiltered'\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=model_version,\n",
        "    api_key=openai_api_key,\n",
        "    base_url=openai_base_url\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_kd2YcV-WKL"
      },
      "source": [
        "OPENAI GPT3.5 FOLLOWS A \"role\" style which is a good way to isolate the main query from the general instructions we want the model to follow. So lets DESIGN a QUERY in the same manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HUdQdeM94TMm"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a helpful assistant.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"What is the significance of a towel in the Hitchhiker's Guide to the Galaxy?\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jhKp_ZI-fEC"
      },
      "source": [
        "LETS GET A RESPONSE FROM THE INITIALIZED chat INSTANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKAty0oN5ijj",
        "outputId": "5d320326-5070-4d7b-c4a7-20e41ce92ca7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"The significance of a towel in the Hitchhiker's Guide to the Galaxy lies in its multifaceted utility. In the story, a towel is described as one of the most useful items a hitchhiker can have. It symbolizes practicality, resourcefulness, and preparedness in the face of the unknown. A towel can serve various purposes, from drying off after a swim to makeshift bedding or even a weapon in times of need. It underscores the theme of adaptability and the importance of being ready for anything in a universe full of surprises.\", response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 24, 'total_tokens': 136}, 'model_name': 'gpt-3.5-unfiltered', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5e058df0-f705-474a-a157-8454b5e9060d-0')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MegKjGr6-iUY"
      },
      "source": [
        "IT RAN JUST FINE. BUT WE NEED TO COME UP WITH A STRUCTURE WHERE WE CAN PROVIDE SYSTEM INSTRUCTIONS ONCE (CHANGE WHEN REQUIRED) AND QUERY AS MANY TIMES AS REQUIRED WITHOUT CREATING THE MESSAGES list EVERYTIME WE WANT TO DO SO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdUrGC0Bw0Un"
      },
      "source": [
        "INTRODUCING ChatTemplate. CREATING A TEMPLATE (HOLLOW STRUCTURE) IN WHICH WE CAN HAVE A COMMON SYSTEM MESSAGE AND CAN FILL THE QUERY PLACEHOLDER ANYTIME VERY EASILY."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GamPjscQ5rCH"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = 'You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses.'\n",
        "human_template = '{query}'\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_template),\n",
        "    (\"human\", human_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2X0jAntjcXx"
      },
      "source": [
        "LANGCHAIN EXPRESSION LANGUAGE\n",
        "\n",
        "| <- used for chaining components\n",
        "\n",
        "Means COMPONENT ON THE LEFT WILL BE SENT TO THE RIGHT (ONE BY ONE IF MULTIPLE COMPONENTS ARE CHAINED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "obY6qv3oBES8"
      },
      "outputs": [],
      "source": [
        "chain = template | model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yuo8r58KxLAh"
      },
      "source": [
        "WE PROVIDED THE HUMAN MESSAGE WITH {query}, WHICH WE CAN REPLACE JUST BEFORE SENDING OUR QUERY TO THE MODEL LIKE BELOW -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOrbQZjyB6zQ",
        "outputId": "eb86a3fc-7c3f-4fd7-fd1b-418f8da1182e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Oh, young apprentice seeking Python wisdom, listen closely to the whispers of the moldy Roquefort and the aged Gouda. To ascend in Python sorcery, embrace the art of code incantations and the dance of the indented realms. Delve deep into the cryptic scrolls of Pecorino Romano, where loops and functions intertwine like a rich Camembert. Remember, young Padawan, practice with zeal, debug with the sharpness of a Parmigiano Reggiano knife, and let the spirit of the Pythonic serpents guide your way. In the land of Python, errors are but stepping stones to enlightenment, and libraries are the sacred cheeses that nourish your creations. May your variables be as versatile as a Swiss cheese and your logic as sharp as a Cheddar. Embrace the journey, for in the world of Python, the quest for mastery is as endless as the varieties of cheese in the grand fromagerie of life.', response_metadata={'token_usage': {'completion_tokens': 199, 'prompt_tokens': 42, 'total_tokens': 241}, 'model_name': 'gpt-3.5-unfiltered', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dc4f400e-de35-4198-9fd6-02488a2854a5-0')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"query\" : \"Could I please have some advice on how to become a better Python Programmer?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr0yncqEj_a2"
      },
      "source": [
        "LETS QUERY FOR THE INFORMATION NOT LEARNED BY THE MODEL DURING ITS TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Q7oyVYuIl7U_"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "You are a helpful AI assistant.\"\"\"\n",
        "human_message = \"\"\"\n",
        "{query}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "V0Afvlg1mGBl"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_message),\n",
        "    (\"human\", human_message)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uoZkeNoEmM1a"
      },
      "outputs": [],
      "source": [
        "chat_chain_no_context = chat_template | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD30iJMUqGby",
        "outputId": "f2428880-96bf-4772-aaa6-3b9f9c9d26d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I was trained on a mixture of licensed data, data created by human trainers, and publicly available data. This corpus was used to pre-train me on a range of language tasks, and the data used for my training is up to date as of September 2021.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 21, 'total_tokens': 74}, 'model_name': 'gpt-3.5-unfiltered', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b7565681-3933-4bef-b701-a4f9fb1e9e26-0')"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_chain_no_context.invoke({\"query\": \"Last date of the training data you were trained with ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfdfyHc8q7MY"
      },
      "source": [
        "And [according to WikiPedia article](https://en.wikipedia.org/wiki/LangChain), **initial release date** for langchain was **Oct 2022**\n",
        "\n",
        "AND **LangChain Expression Language** was **introduced in October 2023**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSdMFDf5rpSo"
      },
      "source": [
        "LETS ASK MODEL ABOUT LANGCHAIN EXPRESSION LANGUAGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_anPFW7BmRwA",
        "outputId": "254ad185-9078-4237-b62d-64c59930be03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"LANGCHAIN EXPRESSION LANGUAGE\\n\\nLangChain Expression Language is a versatile tool for developers. It simplifies coding tasks. The language features a range of functionalities. It supports various data types and operations. Developers find it user-friendly. It aids in creating efficient code. LangChain Expression Language enhances productivity. Its syntax is intuitive. Developers can quickly grasp its concepts. The language promotes clean and concise code. It encourages good coding practices. Its versatility is a standout feature. Developers appreciate its flexibility. They can achieve complex tasks with ease. LangChain Expression Language is a valuable asset. Developers rely on it for diverse projects. It streamlines the development process. The language's efficiency is commendable. It boosts overall coding efficiency. LangChain Expression Language is a preferred choice. Developers enjoy working with its features. It fosters creativity and innovation. The language continues to evolve. Developers eagerly anticipate its updates. LangChain Expression Language is a game-changer in the coding world.\", response_metadata={'token_usage': {'completion_tokens': 203, 'prompt_tokens': 17, 'total_tokens': 220}, 'model_name': 'gpt-3.5-unfiltered', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-257c4a33-738c-46dd-8d6f-9e17225d5d85-0')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_chain_no_context.invoke({\"query\": \"What is LangChain Expression Language ?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOboy7LWyI-0"
      },
      "source": [
        "AS YOU CAN SEE ABOVE, IT hallucinated which MEANS THAT IT PROVIDED AN ANSWER BUT IT IS NOT THE FACTUAL TRUTH AND CAN BE VERIFIED BY LOOKING AT THE INFO AT [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3uMtUF_ympk"
      },
      "source": [
        "* SO WE ARE GOING TO PROVIDE SOME FACTUAL INFORMATION TO THE MODEL ALONG WITH OUR HUMAN MESSAGE and WILL ASK THE MODEL TO USE ONLY THAT INFO TO ANSWER THE QUERY.\n",
        "* IF THE INFO DOESN'T CONTAIN ANY HELPFUL STUFF RELATED TO THE QUERY, WE CAN ASK THE MODEL TO SIMPLE SAY \"I don't know\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HLhirPBHCDky"
      },
      "outputs": [],
      "source": [
        "system_message_w_context = \"\"\"\n",
        "You are a helpful AI assistant. You answer the query by taking only the context in account.\n",
        "If the context does not provide the relevant information, say \"I don't know\".\n",
        "\"\"\"\n",
        "human_message_w_context = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "#QUERY:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "context = \"\"\"\n",
        "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
        "\n",
        "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.\n",
        "\n",
        "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
        "\n",
        "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
        "\n",
        "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "9cMxkzUdlKkr"
      },
      "outputs": [],
      "source": [
        "chat_template_w_context = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_message_w_context),\n",
        "    (\"human\", human_message_w_context)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hv5XMNmDllLa"
      },
      "outputs": [],
      "source": [
        "chat_chain_w_context = chat_template_w_context | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M5YF1yOlrFQ",
        "outputId": "8e2c7612-1acb-44ff-d4b3-54b7739a5b21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LANGCHAIN EXPRESSION LANGUAGE (LCEL)\\n\\nLangChain Expression Language (LCEL) is a declarative method for effortlessly combining chains. This approach offers various advantages over traditional coding methods. LCEL provides support for asynchronous, batch, and streaming operations by default, simplifying the process of prototyping chains in tools like Jupyter notebooks and transitioning them into asynchronous streaming interfaces seamlessly.\\n\\nFALLBACKS AND ERROR HANDLING\\n\\nOne crucial aspect of LCEL is its ability to handle errors gracefully, particularly in the context of the inherent non-determinism of Large Language Models (LLMs). By incorporating fallback mechanisms, LCEL allows for easy error recovery and fault tolerance within chains.\\n\\nPARALLELISM AND EFFICIENCY\\n\\nIn LLM applications, where tasks often involve time-consuming API calls, the need for parallel execution is common. LCEL syntax inherently supports parallel processing, enabling components that can run concurrently to do so efficiently, enhancing overall performance.\\n\\nTRACEABILITY AND DEBUGGING WITH LANGSMITH\\n\\nAs chains grow in complexity, monitoring and understanding their behavior at each step becomes vital. LCEL offers seamless integration with LangSmith tracing, ensuring that every action within the chain is automatically logged. This feature enhances observability and facilitates effective debugging of intricate chain operations.', response_metadata={'token_usage': {'completion_tokens': 284, 'prompt_tokens': 293, 'total_tokens': 577}, 'model_name': 'gpt-3.5-unfiltered', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5e78ac85-851a-45e8-a35c-8d539765a10f-0')"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_chain_w_context.invoke({\"query\": \"What is LangChain Expression Language ?\", \"context\": context})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVGc-lxEsPLq"
      },
      "source": [
        "THERE IT IS. IT USED THE PROVIDED CONTEXT (INFORMATION) TO ANSWER THE QUERY."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03dsvMSK3w3S"
      },
      "source": [
        "**Allmost all LLMs have a limited context window** which is typically **measured in tokens**. This window is an upper bound of how much stuff we can stuff in the model's input at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwOHaOKV3_L7",
        "outputId": "765bfcdd-9707-4810-cbce-a789237ec114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tiktoken.list_encoding_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "X7KEgRcJ4rRX"
      },
      "outputs": [],
      "source": [
        "model_for_encoding = 'gpt-3.5-turbo'    # this works fine for \"model_version\" we are using\n",
        "\n",
        "encoding = tiktoken.encoding_for_model(model_name=model_for_encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf2NeHMW5Tnj",
        "outputId": "af6fe471-8abc-4315-cbb3-e179aa493e53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lets check encoded context\n",
        "len(encoding.encode(context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-Mpe2s55wP_"
      },
      "source": [
        "THE MODEL \"**gpt-3.5-unfiltered**\" HAS **CONTEXT WINDOW OF 4096 tokens** which comes around **750 - 1000 words in english vocabulary**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84aAzoEqqMH"
      },
      "source": [
        "CONTEXT CAN BE HUGE (1000s of words) BUT THEY DO NOT NEED TO BE. PROVIDING HUGE INFORMATION IN CONTEXT IS\n",
        "\n",
        "*   first of all NOT SUPPORTED BY ALL MODELS\n",
        "*   second, NOT ALL INFORMATION MAY BE RELEVANT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rOke1YjsXqD"
      },
      "source": [
        "ONE METHOD CAN BE TO CREATE SMALL SIZE INFORMATION UNITS (chunks), SELECT THE RELEVANT ONES AND PROVIDE THEM IN CONTEXT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMkciMnHuPSV"
      },
      "source": [
        "* COMPARING STRING CHUNKS WITH QUERY DIRECTLY IS NOT AT ALL A GOOD WAY TO GO BECAUSE THAT WILL REQUIRE US TO CODE ALL POSSIBLE COMBINATIONS OF WORDS AND MEANINGS WHICH IS IMPOSSIBLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCLvZEKXujwb"
      },
      "source": [
        "* AND, STORING THE CHUNKS DIRECTLY IS ALSO NOT A GOOD STRATEGY BECAUSE IT MAY REQUIRE HUGE AMOUNT OF MEMORY DEPENDING ON THE USECASE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aDF-QsBvIRA"
      },
      "source": [
        "* WE NEED TO FIND AN APPROACH TO STORE THE CHUNKS IN AN EFFICIENT MANNER WHILE CONSIDERING THE ABILITY TO PERFORM COMPUTATIONS ON AND WITH IT ALSO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnrLFskmvLV-"
      },
      "source": [
        "* TAKING INTO CONSIDERATION THE FACT THE COMPUTERS ARE AMAZING AT PERFORMING OPERATIONS WITH NUMBERS, WE ARE GOING TO SOMEHOW CONVERT OUR STRING CHUNKS TO VECTORS WHICH WILL ALLOW US TO STORE TWO SET OF INFORMATION AT A TIME AS WELL AS ENABLE US WITH FACILITY TO COMPUTE SOMETHING ON AND WITH IT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTiED3mQcO6s"
      },
      "source": [
        "PARTICULARLY THERE ARE 2 COMMON DATA REPRESENTATIONS CAPTURING SEMANTIC INFORMATION -\n",
        "\n",
        "\n",
        "*   DENSE\n",
        "*   SPARSE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2yG5MO5c_jK"
      },
      "source": [
        "**DENSE** - WHEN WE HAVE HUGE NUMBER OF DATA POINTS AND WE WANT TO CAPTURE AND COMPUTE SEMANTIC RELATEDNESS BETWEEN THEM AS WELL AS THE INCOMING DATA\n",
        "\n",
        "**SPARSE** - WHEN WE HAVE LESS NUMBER OF DATA POINTS AND WANT TO CATEGORIZE THE DATA POINTS AS WELL AS THE INCOMING DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0WeMtR6wG5O"
      },
      "source": [
        "* THERE ARE MANY WAYS STRING(s) CAN BE CONVERTED TO NUMBERS BUT NOT ALL WAYS ARE EFFICIENT. THE TO-BE SELECTED WAY SHOULD BE ABLE TO PERFORM CONVERSION IN AN EFFICIENT MANNER AS WELL AS SHOULD NOT BE TOO COMPUTATIONALLY COMPLEX AND TIME-TAKING AS WELL AS SHOULD NOT UTILIZE TOO MUCH STORAGE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbfZGMGhyVZW"
      },
      "source": [
        "* THE APPROACH SHOULD BE ABLE TO EXPRESS semantic relatedness (e.g. \"boat\" - \"water\") AND semantic similarity (e.g. \"boat\" - \"ship\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0x2kPjsxzbM"
      },
      "source": [
        "* ONE VERY GOOD WAY IS COMPUTING \"Word embeddings\" which uses vectors. Word embeddings are based on the idea that contextual information alone constitutes a viable representation of linguistic items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLs_BlWw25tZ"
      },
      "source": [
        "### VECTORS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "435NMwCZu6Cz"
      },
      "source": [
        "* Vectors are built from components, which are ordinary numbers. You can think of a vector as a list of numbers, and vector algebra as operations performed on the numbers in the list.\n",
        "\n",
        "* **In python, a vector can be represented as a tuple of scalars** (ordinary numbers INT or FLOAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CRq59mBvV_s"
      },
      "source": [
        "Vector arithmetic (mainly dot product) can be used for calculating vector projections, vector decompositions, and determining orthogonality.\n",
        "\n",
        "*   vector projection - vector projection gives the magnitude of projection of one vector over another vector. The vector projection is a scalar value. The vector projection of one vector over another is obtained by multiplying the given vector with the cosine of the angle between the two vectors.\n",
        "\n",
        "*   vector decomposition - vector decomposition is vector addition in reverse. Usually we decompose vectors into component vectors that are orthogonal (90deg). This must be done in such a way that the component vectors sum to the original vector.\n",
        "\n",
        "*   orthogonality - generalization of the notion of perpendicularity\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27lUbyY1SdC"
      },
      "source": [
        "* Here's a good **HISTORY** AND overview of how things evolved over time to [\"Word embeddings\"](https://www.gavagai.io/text-analytics/a-brief-history-of-word-embeddings/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjpNqtP6yp9Z"
      },
      "source": [
        "* THERE ARE MANY VERY FAMOUS SOLUTIONS like [word2vec](https://github.com/danielfrg/word2vec), [sentence_transformers](https://www.sbert.net/) and a lot more. They are benchmarked and listed on [MTEB(Massive text embedding benchmark)](https://huggingface.co/spaces/mteb/leaderboard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm-exXmVz5zl"
      },
      "source": [
        "* We will use [sentence-transformers/paraphrase-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2) which provides a good tradeoff between speed of operations and representational accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3yd3-vIu6Ie"
      },
      "source": [
        "Lets try encoding some text now to get a feel of what an embedding(output) output looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q50P-yF2vFoW",
        "outputId": "4feb6a61-5f92-4187-eb5b-12a72632da12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LN6yg-90P8C"
      },
      "source": [
        "* This **sentence-transformers/paraphrase-MiniLM-L6-v2** is a model hosted on [huggingface](https://huggingface.co/) and since we are using langchain for almost everything, we will use [langchain-huggingface](https://pypi.org/project/langchain-huggingface/) from pypi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_gZonD9z54P"
      },
      "source": [
        "LETS TRY CREATING EMBEDDINGS FOR SIMPLE TERMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ticoxh9_uLEv",
        "outputId": "86cb73ed-f440-498c-899b-f749ca8108fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "puppy_embedding (vector) length: 384\n",
            "dog_embedding (vector) length: 384\n"
          ]
        }
      ],
      "source": [
        "puppy_embedding = embedding_model.embed_query('puppy')\n",
        "dog_embedding = embedding_model.embed_query('dog')\n",
        "\n",
        "print(\"puppy_embedding (vector) length:\", len(puppy_embedding))\n",
        "print(\"dog_embedding (vector) length:\", len(dog_embedding))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHe1Q6bk0Ze7"
      },
      "source": [
        "THE LENGTH OF THE EMBEDDING REPRESENTS THE DIMENSION DEPTH IN WHICH THE WORD/QUERY IS REPRESENTED.\n",
        "EVERY MODEL HAS REPRESENTS A QUERY IN SOME DIMENSION DEPTH which USUALLY RANGES FROM 384 to 1586 in some cases.\n",
        "HERE \"puppy\" and \"dog\" ARE REPRESENTED IN 384 DIMENSIONS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLKfHyyw083n"
      },
      "source": [
        "SO THE EMBEDDING MODEL HAS REPRESENTED THE QUERY IN MULTI-DIMENSIONAL SPACE. BUT WAS IT ABLE TO CAPTURE THE SEMANTIC RELATEDNESS AND SIMILARITY IN ANY WAY ?\n",
        "\n",
        "LETS FIND OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XL5FAKQo1Mfs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "  return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6TgOIc91aG3",
        "outputId": "227e6d65-0d0e-4b84-af85-751c349ba8c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8027491357540261"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(puppy_embedding, dog_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9iyNDNV1gd5"
      },
      "source": [
        "IT GIVES US A FLOATING POINT NUMBER FROM -1 to 1.\n",
        "\n",
        "*   CLOSER TO -1 being not very related\n",
        "*   CLOSER TO 1 means highly likely related"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUub125y11h7"
      },
      "source": [
        "JUST FOR FUN, LETS TRY TO COMPUTE THE SIMILARITY OF \"puppy\" and \"planet jupiter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7DO6Sws1_EX",
        "outputId": "74e7ab50-19b5-48d4-9cee-71ec0c64a35f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.03535227699084348"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "planet_jupiter_embedding = embedding_model.embed_query('planet jupiter')\n",
        "\n",
        "cosine_similarity(puppy_embedding, planet_jupiter_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwxs9jQv2SdH"
      },
      "source": [
        "SEE ? IT BEAUTIFULLY SHOWS HOW UNRELATED a PUPPY AND PLANET JUPITER ARE.\n",
        "\n",
        "THATS WHAT WE WANTED RIGHT ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVUN1V53qMEv"
      },
      "source": [
        "* CREATING SMALL SIZE INFORMATION UNITS (chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "kOk8whP-ssLr"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = encoding.encode(text)\n",
        "    return len(tokens)\n",
        "\n",
        "chunk_creator = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len\n",
        "    )\n",
        "\n",
        "chunks = chunk_creator.split_text(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "nFHGCAabtr7S",
        "outputId": "d59dc6e3-c619-49d3-8395-608bd6595f75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\\n\\nAsync, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_DSkWlK6xZl"
      },
      "source": [
        "LETS GET THE RELEVANT CHUNK FOR A QUERY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr3fsA557FIL"
      },
      "source": [
        "* CONVERTING CHUNK TO EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "J-v1XInr6wlX"
      },
      "outputs": [],
      "source": [
        "embeddings_dict = {}\n",
        "\n",
        "for chunk in chunks:\n",
        "  embeddings_dict[chunk] = embedding_model.embed_query(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE6mydKO7Nnf"
      },
      "source": [
        "* PRINT CHUNKS ALONG WITH THEIR EMBEDDING SIZES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdN9E-XC7Sm0",
        "outputId": "f57b0920-b17b-420a-daa2-016d0483f716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface. \n",
            "\n",
            "Vector Embedding Length : 384\n",
            "------------------------------------------------------------\n",
            "Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.\n",
            "\n",
            "Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are. \n",
            "\n",
            "Vector Embedding Length : 384\n",
            "------------------------------------------------------------\n",
            "Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability. \n",
            "\n",
            "Vector Embedding Length : 384\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for chunk, embedding in embeddings_dict.items():\n",
        "  print(chunk, '\\n')\n",
        "  print(\"Vector Embedding Length :\", len(embedding))\n",
        "  print(\"------\"*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejPRAvRO9D-b"
      },
      "source": [
        "EMBED THE QUERY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "CfFM1JVK9F9n"
      },
      "outputs": [],
      "source": [
        "rag_query = \"Can LCEL help take code from the notebook to production?\"\n",
        "\n",
        "query_vector = embedding_model.embed_query(rag_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSmrhcSg80Lv"
      },
      "source": [
        "GET THE MOST SIMILAR CHUNK TO THE QUERY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsx6FsEM70P9",
        "outputId": "50b2e5ca-45c9-4279-f104-f8ce4be0cdf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Can LCEL help take code from the notebook to production? \n",
            "\n",
            "Most Similar Chunk:  LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface. \n",
            "\n",
            "Similarity Score: 0.5815063597022642\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def retrieve_similar_content(query_vector, embeddings_dict):\n",
        "    max_similarity_score = 0\n",
        "    most_similar_chunk = ''\n",
        "\n",
        "    for chunk, chunk_vector in embeddings_dict.items():\n",
        "        chunk_similarity_score = cosine_similarity(chunk_vector, query_vector)\n",
        "\n",
        "        if max_similarity_score < chunk_similarity_score:\n",
        "          max_similarity_score = chunk_similarity_score\n",
        "          most_similar_chunk = chunk\n",
        "\n",
        "    print(\"Query:\", rag_query, '\\n')\n",
        "    print(\"Most Similar Chunk: \", most_similar_chunk, '\\n')\n",
        "    print(\"Similarity Score:\", max_similarity_score)\n",
        "    print(\"-----\"*10)\n",
        "\n",
        "    return most_similar_chunk\n",
        "\n",
        "_ = retrieve_similar_content(query_vector, embeddings_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XocCxCwh-b1Z"
      },
      "source": [
        "LETS CREATE A CHAIN WHICH WILL PERFORM THESE OPERATIONS -\n",
        "\n",
        "* EMBED QUERY\n",
        "* GET THE MOST SEMANTICALLY SIMILAR CHUNK\n",
        "* PROVIDE THE CHUNK IN CONTEXT\n",
        "* GET THE ANSWER FROM THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "FedjNqHR-Zmg"
      },
      "outputs": [],
      "source": [
        "def create_query_vector(query):\n",
        "    return embedding_model.embed_query(query)\n",
        "\n",
        "def create_chat_template(chunk):\n",
        "    system_message = (\n",
        "        \"You are a helpful AI bot. You respond to the query provided by \"\n",
        "        \"the user by only using information provided in the context. If \"\n",
        "        \"there is no information related to the user query in the context\"\n",
        "        \"\"\" then you just respond as \"I don't know.\".\"\"\"\n",
        "    )\n",
        "    human_message = (\n",
        "        f\"#CONTEXT:\\n{chunk}\\n\\n\\n\"\n",
        "        \"#QUERY:\\n{query}\\n\"\n",
        "    )\n",
        "    template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_message),\n",
        "        (\"human\", human_message)\n",
        "    ])\n",
        "\n",
        "    return template\n",
        "\n",
        "def create_chain(template, model):\n",
        "    chain = template | model\n",
        "    return chain\n",
        "\n",
        "def get_response_from_rag(query):\n",
        "    query_vector = create_query_vector(query)\n",
        "    most_similar_chunk = retrieve_similar_content(query_vector, embeddings_dict)\n",
        "    template = create_chat_template(most_similar_chunk)\n",
        "    chain = create_chain(template, model)\n",
        "    model_response = chain.invoke({\"query\": query})\n",
        "    return model_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF9E4fCXLxkh",
        "outputId": "6ff1b83c-6ea1-48c8-c4ef-ed50030600e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Can LCEL help take code from the notebook to production? \n",
            "\n",
            "Most Similar Chunk:  LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):\n",
            "\n",
            "Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface. \n",
            "\n",
            "Similarity Score: 0.5815063597022642\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LCEL can indeed aid in transitioning code from a notebook to production. By utilizing LangChain Expression Language, developers can seamlessly compose chains with asynchronous, batch, and streaming support. This feature facilitates the quick prototyping of chains within a Jupyter notebook using the synchronous interface. Subsequently, developers can effortlessly transition these chains to production by exposing them through an asynchronous streaming interface. This streamlined process enhances the efficiency of moving code from the development stage in a notebook environment to a production-ready state.', response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 172, 'total_tokens': 273}, 'model_name': 'gpt-3.5-unfiltered', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9a42cfaf-fa1d-481c-8498-b9a364170b58-0')"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_query = \"Can LCEL help take code from the notebook to production?\"\n",
        "\n",
        "get_response_from_rag(user_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KFkqn81NQkA"
      },
      "source": [
        "GREAT !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvgbzw03Nulz"
      },
      "source": [
        "NOW LETS TRY DOING THIS ON OUR OWN DOCUMENT !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGSbwRPgNSuH"
      },
      "source": [
        "BUT WAIT. WHAT IF OUR DOCUMENTS ARE HUGE and ARE OF DIVERSE RANGE OF FORMATS ?\n",
        "* pdf, pptx, docx, html, md, txt, csv etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VuaBtsMN3XQ"
      },
      "source": [
        "FIRST WE WILL HAVE TO PARSE AND BRING THEM IN A FORMAT SUITABLE FOR PROCEEDING WITH CHUNKING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3gwy418OAUY"
      },
      "source": [
        "WE'LL DO THIS FOR A PDF and UNDERSTAND HOW THE END-TO-END PIPELINE MAY LOOK LIKE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "AhrgFznfnFOU"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "documents = PyMuPDFLoader(file_path='https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf').load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWmG_1AyOx8q"
      },
      "source": [
        "LETS SPLIT DOCUMENTS IN CHUNKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "EeHqmoIaOkba"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "KA5TZB7AW-eJ"
      },
      "outputs": [],
      "source": [
        "text_chunks = [_chunk.page_content for _chunk in chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRB7ATJIO-Zu"
      },
      "source": [
        "NOW CALCULATING EMBEDDINGS FOR THESE CHUNKS and STORING IT IN A VECTOR STORE (DATABASE FOR VECTORS).\n",
        "* LangChain contains helper libraries for vast variety of vector stores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "FleloB4mO3Hv"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "docs_vector_store = FAISS.from_documents(documents, embedding_model)\n",
        "# chunks_vector_store = FAISS.from_documents(chunks, embedding_model)\n",
        "chunks_vector_store = FAISS.from_texts(text_chunks, embedding_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZlt06h0P7VP"
      },
      "source": [
        "MAKING THIS VECTOR STORE SUITABLE FOR RETRIEVAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "Co9vB_jMPyKJ"
      },
      "outputs": [],
      "source": [
        "doc_retriever = docs_vector_store.as_retriever()\n",
        "chunk_retriever = chunks_vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egIcREoeQKs7"
      },
      "source": [
        "LETS TEST THE VECTOR RETRIEVER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "L7EaXD_6QIP0"
      },
      "outputs": [],
      "source": [
        "query_for_rag = \"What is the significance of towels in Douglas Adam's Hitchhicker's Guide?\"\n",
        "related_doc_chunks = doc_retriever.invoke(query_for_rag)\n",
        "related_text_chunks = chunk_retriever.invoke(query_for_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKRxUZ91QsmF",
        "outputId": "1e863c4c-0077-4277-bbcc-c48f57cba430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='14  /  D O U G L A S  A D A M S  \\nFord wished that a flying saucer would arrive soon because he \\nknew how to flag flying saucers down and get lifts from them.  \\nHe knew how to see the Marvels of the Universe for less than \\nthirty Altairan dollars a day. \\nIn fact, Ford Prefect was a roving researcher for that wholly \\nremarkable book The Hitch Hiker\\'s Guide to the Galaxy. \\nHuman beings are great adaptors, and by lunchtime life in the \\nenvirons of Arthur\\'s house had settled into a steady routine.  It \\nwas Arthur\\'s accepted role to lie squelching in the mud making \\noccasional demands to see his lawyer, his mother or a good book; \\nit was Mr Prosser\\'s accepted role to tackle Arthur with the \\noccasional new ploy such as the For the Public Good talk, the \\nMarch of Progress talk, the They Knocked My House Down \\nOnce You Know, Never Looked Back talk and various other \\ncajoleries and threats; and it was the bulldozer drivers\\' accepted \\nrole to sit around drinking coffee and experimenting with union \\nregulations to see how they could turn the situation to their \\nfinancial advantage. \\nThe Earth moved slowly in its diurnal course. \\nThe sun was beginning to dry out the mud Arthur lay in. \\nA shadow moved across him again. \\n\"Hello Arthur,\" said the shadow.  Arthur looked up and squinting \\ninto the sun was startled to see Ford Prefect standing above him. \\n\"Ford! Hello, how are you?\" \\n\"Fine,\" said Ford, \"look, are you busy?\" \\n', metadata={'source': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'file_path': 'https://www.deyeshigh.co.uk/downloads/literacy/world_book_day/the_hitchhiker_s_guide_to_the_galaxy.pdf', 'page': 13, 'total_pages': 227, 'format': 'PDF 1.3', 'title': \"Hitchhiker's Guide to the Galaxy\", 'author': 'Douglas Adams', 'subject': 'document version 1.0', 'keywords': 'hanomag <01name@iname.com>', 'creator': \"Hitchhiker's Guide to the Galaxy.doc (Preview) - Microsoft Word\", 'producer': 'Acrobat PDFWriter 4.0 for Windows NT', 'creationDate': 'D:20010213123949', 'modDate': \"D:20010213124359-08'00'\", 'trapped': '', 'encryption': 'Standard V1 R2 40-bit RC4'})"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "related_doc_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpmTAEL3ZkqR",
        "outputId": "358fd5e7-275f-44fb-f0ed-f335cdd0778c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=\"14  /  D O U G L A S  A D A M S  \\nFord wished that a flying saucer would arrive soon because he \\nknew how to flag flying saucers down and get lifts from them.  \\nHe knew how to see the Marvels of the Universe for less than \\nthirty Altairan dollars a day. \\nIn fact, Ford Prefect was a roving researcher for that wholly \\nremarkable book The Hitch Hiker's Guide to the Galaxy. \\nHuman beings are great adaptors, and by lunchtime life in the \\nenvirons of Arthur's house had settled into a steady routine.  It \\nwas Arthur's accepted role to lie squelching in the mud making \\noccasional demands to see his lawyer, his mother or a good book; \\nit was Mr Prosser's accepted role to tackle Arthur with the \\noccasional new ploy such as the For the Public Good talk, the \\nMarch of Progress talk, the They Knocked My House Down \\nOnce You Know, Never Looked Back talk and various other \\ncajoleries and threats; and it was the bulldozer drivers' accepted \\nrole to sit around drinking coffee and experimenting with union \\nregulations to see how they could turn the situation to their \\nfinancial advantage. \\nThe Earth moved slowly in its diurnal course.\")"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "related_text_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFVgja7lUoZx",
        "outputId": "d63b557b-fb84-432d-d9a1-ef66d1988bc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='T H E  H I T C H H I K E R \\' S  G U I D E  T O  T H E  G A L A X Y  /  95  \\n\"Alright,\" said Marvin like the tolling of a great cracked bell, \"I\\'ll \\ndo it.\" \\n\"Good ...\" snapped Zaphod, \"great ...  thank you ...\" \\nMarvin turned and lifted his flat-topped triangular red eyes up \\ntowards him. \\n\"I\\'m not getting you down at all am I?\" he said pathetically. \\n\"No no Marvin,\" lilted Trillian, \"that\\'s just fine, really ...\" \\n\"I wouldn\\'t like to think that I was getting you down.\" \\n\"No, don\\'t worry about that,\" the lilt continued, \"you just act as \\ncomes naturally and everything will be just fine.\" \\n\"You\\'re sure you don\\'t mind?\" probed Marvin. \\n\"No no Marvin,\" lilted Trillian, \"that\\'s just fine, really ...  just part \\nof life.\" \\n\"Marvin flashed him an electronic look. \\n\"Life,\" said Marvin, \"don\\'t talk to me about life.\" \\nHe turned hopelessly on his heel and lugged himself out of the \\ncabin.  With a satisfied hum and a click the door closed behind \\nhim')"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunk_retriever.invoke(\"What species does Marvin belong to ?\")[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yG1JXPiRtVG"
      },
      "source": [
        "LETS COMPLETE THE RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ALb9cPF0QulA"
      },
      "outputs": [],
      "source": [
        "HUMAN_MESSAGE = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, response with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(HUMAN_MESSAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TdxU7jFSxPb"
      },
      "source": [
        "LETS USE THE LangChain LCEL AND CHAIN OUR COMPONENTS TOGETHER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "rMyy4_vkR0WE"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": doc_retriever, \"query\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | model\n",
        "    | StrOutputParser(name='content')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "F93Br8vfSON_",
        "outputId": "c2f83506-55a4-4070-fa2b-378734990b7d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SIGNIFICANCE OF TOWELS IN DOUGLAS ADAMS\\' HITCHHIKER\\'S GUIDE:\\n\\nTowels hold a special significance in Douglas Adams\\' Hitchhiker\\'s Guide to the Galaxy. In the book, it is mentioned that a towel is one of the most useful things a hitchhiker can have. The exact lines from the book are as follows: \"A towel, it says, is about the most massively useful thing an interstellar hitchhiker can have. Partly it has great practical value.\" Towels are emphasized for their practicality and versatility, acting as a multipurpose tool for hitchhikers in the vastness of space.\\n\\nIn addition to being useful for drying oneself, towels in the Hitchhiker\\'s Guide serve various other purposes, highlighting their importance in the galactic journey. The book humorously describes the significance of towels in a humorous and memorable way, adding a unique and quirky element to the narrative.'"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"What is the significance of towels in Douglas Adam's Hitchhicker's Guide? Provide the answer and also print the exact lines from which you got the answer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06_JLE77Skum"
      },
      "source": [
        "COOL !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zoD7SzQuUBM8",
        "outputId": "f6591893-42c9-4fb7-faf0-dbb84efa5ab5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ford is from Betelgeuse Seven.'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"What is the name of the planet Ford is from?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QVrsvCO5UX9f",
        "outputId": "23a44dd7-3dc3-4406-e37c-96c6ab3e30a2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Zaphod sent Trillian to escort the hitchhikers to the bridge.'"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"Did Zaphod send Marvin or Trillion, to escort the hitchhikers to the bridge ?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf-AkDKJSmrG"
      },
      "source": [
        "LETS TEST IT OUT FOR A QUERY FOR WHICH WE HAVE NO DATA IN THE VECTOR STORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ow1sc1r2SU3Q",
        "outputId": "11bd0360-eebd-4d45-bb2c-970bd007368b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke(\"What is the airspeed velocity of an unladen swallow?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Z9NBuySuaW"
      },
      "source": [
        "PERFECT !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew0cXCYXSsG9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
